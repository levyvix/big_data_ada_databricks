# Projeto Big Data usando AWS e Databricks

## Descri√ß√£o
Este reposit√≥rio cont√©m um projeto de Big Data desenvolvido utilizando AWS e Databricks. O objetivo √© demonstrar o uso dessas tecnologias para o processamento e an√°lise de grandes volumes de dados.

## Estrutura do Projeto
- **notebooks/**: Cont√©m os notebooks Jupyter usados para a an√°lise e visualiza√ß√£o de dados.
- **src/**: C√≥digo-fonte do projeto, incluindo scripts de ETL (Extract, Transform, Load).
- **tests/**: Testes automatizados para garantir a qualidade do c√≥digo.
- **.gitignore**: Arquivo que especifica quais arquivos ou diret√≥rios devem ser ignorados pelo Git.
- **LICENSE**: Licen√ßa do projeto.
- **README.md**: Este arquivo.

## Tecnologias Utilizadas
- **AWS**: Para armazenamento e processamento de dados na nuvem.
- **Databricks**: Plataforma unificada de an√°lise de dados.
- **Python**: Linguagem de programa√ß√£o utilizada nos scripts e notebooks.


## Arquitetura

### Medallion Architecture

![architecture](images/architecture.jpeg)

A arquitetura 'Medalion' ou Medalh√£o, √© uma arquitetura de dados que combina os recursos da AWS e da Databricks para fornecer uma solu√ß√£o completa para o processamento e an√°lise de grandes volumes de dados.

√â um padr√£o de design de dados, criado pelo Databricks, usado para organizar logicamente os dados em um LakeHouse, com o objetivo de melhorar incrementalmente a qualidade dos dados √† medida que eles fluem por varias camadas.

Esta arquitetura consiste em tr√™s camadas - bronze (bruto), prata (validado) e ouro (enriquecido) - cada uma representa niveis progressivamente mais altos de qualidade.

### Est√°gios do 'Medallion'

### ü•âBronze

A camada bronze serve como o ponto inicial para a ingest√£o e armazenamento de dados. Os dados s√£o salvos sem processamento ou transforma√ß√£o. Isso pode incluir o salvamento de logs de uma aplica√ß√£o em um sistema de arquivos distribu√≠do ou eventos de streaming do Kafka.

### ü•àPrata

A camada prata √© onde as tabelas s√£o limpas, filtradas ou transformadas em um formato mais utiliz√°vel. As transforma√ß√µes nesta etapa devem ser modifica√ß√µes leves, n√£o agrega√ß√µes ou enriquecimentos. No nosso exemplo inicial, esses logs podem ser analisados para extrair informa√ß√µes √∫teis ‚Äî como desaninhamento de estruturas ou elimina√ß√£o de abrevia√ß√µes. Os eventos podem ser padronizados para unificar conven√ß√µes de nomenclatura ou dividir um √∫nico fluxo em v√°rias tabelas.


### ü•áOuro

Finalmente, na etapa ouro, os dados s√£o refinados para atender a requisitos espec√≠ficos de neg√≥cios e an√°lises. Isso pode significar agregar dados em uma granularidade espec√≠fica, como di√°ria ou hor√°ria, ou enriquecer os dados com informa√ß√µes de fontes externas. Ap√≥s a etapa ouro, os dados devem estar prontos para serem consumidos por equipes downstream, como an√°lises, ci√™ncia de dados ou opera√ß√µes de Machine Learning.

## Tecnologias

### AWS S3


<img src="images/s3.png" width="100">

O Amazon S3 (Simple Storage Service) √© um servi√ßo de armazenamento de objetos oferecido pela AWS (Amazon Web Services). Ele permite que os usu√°rios armazenem e recuperem qualquer quantidade de dados a qualquer momento, de qualquer lugar na web. Os dados s√£o armazenados em "buckets" (baldes), que s√£o reposit√≥rios de armazenamento com um nome exclusivo dentro da AWS. Cada objeto dentro de um bucket √© identificado por uma chave √∫nica (um nome) e pode conter at√© 5 TB de dados.

### Principais caracter√≠sticas do Amazon S3

- **Escalabildiade**: O S3 √© altamente escal√°vel, suportando um volume massivo de dados e um n√∫mero muito grande de solicita√ß√µes simult√¢neas.

- **Durabilidade**: Os dados s√£o replicados automaticamente em m√∫ltiplas regi√µes, garantindo uma durabilidade de 99,999999999% (11 noves).

- **Seguran√ßa**: O S3 oferece v√°rias op√ß√µes de seguran√ßa, incluindo criptografia de dados em repouso e em tr√¢nsito, al√©m de controles de acesso granular por meio de pol√≠ticas de bucket e listas de controle de acesso (ACLs).

- **Custo-beneficio**: O S3 opera com um modelo de pagamento conforme o uso, onde voc√™ paga apenas pelo armazenamento e transfer√™ncia de dados que realmente utiliza.

- **Integra√ß√£o**: O S3 se integra facilmente com outros servi√ßos da AWS, como Lambda, EMR, Athena, e muitos outros, facilitando a cria√ß√£o de solu√ß√µes robustas e complexas.

### Uso do Amazon S3 em uma arquitetura de LakeHouse

- **Armazenamento Centralizado de Dados Brutos**: O S3 √© usado para armazenar grandes volumes de dados brutos provenientes de v√°rias fontes (bases de dados, logs, streams de eventos, etc.). Esses dados s√£o armazenados em seu formato original e n√£o estruturado, permitindo uma ingest√£o r√°pida e eficiente.

- **Camada de Dados Curados**: Ap√≥s o processamento e a limpeza dos dados brutos, os dados curados s√£o armazenados novamente no S3 em formatos otimizados para consulta, como Parquet ou ORC. Isso facilita consultas r√°pidas e eficientes, aproveitando ferramentas de processamento distribu√≠do.

- **Integra√ß√£o com Ferramentas de An√°lise e BI**: Ferramentas como AWS Athena, que permite consultas SQL diretamente sobre os dados armazenados no S3, ou AWS Glue, que facilita a cataloga√ß√£o e transforma√ß√£o de dados, podem ser usadas para acessar e analisar os dados armazenados no S3 sem a necessidade de mover os dados para outro reposit√≥rio.

- **Data Lake com Governan√ßa e Seguran√ßa**: Com o uso de AWS Lake Formation, √© poss√≠vel implementar governan√ßa de dados sobre o S3, definindo pol√≠ticas de acesso e monitorando o uso dos dados, garantindo conformidade com regulamentos e pr√°ticas de seguran√ßa.

- **Custo e Efici√™ncia**: O S3 permite o armazenamento escal√°vel e econ√¥mico de dados, com a possibilidade de configurar diferentes classes de armazenamento (Standard, Intelligent-Tiering, Glacier) conforme a frequ√™ncia de acesso e necessidade de recupera√ß√£o de dados.

## Databricks
<img src="images/databricks logo.png" width="150">

O *Databricks* √© uma plataforma de dados unificada que combina engenharia de dados, ci√™ncia de dados, e an√°lise de dados, facilitando a cria√ß√£o e gest√£o de solu√ß√µes de big data e intelig√™ncia artificial. Foi criado pelos fundadores do Apache Spark e √© amplamente utilizado para processar e analisar grandes volumes de dados de maneira eficiente.

### Principais caracter√≠sticas do Databricks:

1. **Apache Spark Simplificado**: O Databricks oferece uma interface simplificada para trabalhar com o Apache Spark, facilitando o desenvolvimento e a execu√ß√£o de pipelines de dados e an√°lises complexas.

2. **Colabora√ß√£o em Equipe**: Com recursos integrados de colabora√ß√£o, como notebooks compartilhados e controle de vers√£o, o Databricks permite que equipes colaborem de forma eficaz em projetos de an√°lise de dados.

3. **Escalabilidade e Desempenho**: O Databricks √© altamente escal√°vel e oferece um desempenho excepcional para processamento de grandes volumes de dados, gra√ßas ao seu suporte ao Apache Spark distribu√≠do.

4. **Integra√ß√£o com Ecossistema de Big Data**: Al√©m do Apache Spark, o Databricks se integra facilmente com outros componentes do ecossistema de big data, como o Delta Lake, para armazenamento de dados, e o MLflow, para gerenciamento de fluxos de trabalho de machine learning.

### Uso do Databricks em uma arquitetura de Lakehouse:

1. **Processamento de Dados em Larga Escala**: O Databricks √© utilizado para processar grandes volumes de dados em um ambiente distribu√≠do, aproveitando a capacidade de processamento do Apache Spark.

2. **Prepara√ß√£o e Limpeza de Dados**: Equipes de dados utilizam o Databricks para preparar e limpar dados brutos antes de armazen√°-los no Data Lake. Isso inclui transforma√ß√µes de dados, deduplica√ß√£o, e outras opera√ß√µes de limpeza.

3. **An√°lise e Explora√ß√£o de Dados**: Cientistas de dados e analistas utilizam o Databricks para realizar an√°lises explorat√≥rias sobre os dados armazenados no Data Lake, utilizando recursos como SQL, Python, e R.

4. **Treinamento de Modelos de Machine Learning**: O Databricks oferece suporte para treinamento de modelos de machine learning em larga escala, utilizando o Apache Spark e integrando-se com bibliotecas populares de machine learning, como o TensorFlow e o PyTorch.

5. **Gerenciamento de Metadados e Governan√ßa**: O Databricks fornece recursos para gerenciamento de metadados, rastreamento de proveni√™ncia e governan√ßa de dados, ajudando as organiza√ß√µes a garantir a qualidade e a seguran√ßa dos dados armazenados no Data Lake.

## Python

<img src="images/python logo.png" width="100">

### Por que o Python √© utilizado como ingest√£o em uma arquitetura de Lakehouse:

- **Flexibilidade**: Python √© uma linguagem de programa√ß√£o extremamente flex√≠vel, o que a torna adequada para lidar com uma variedade de fontes de dados e formatos de dados diferentes.

- **Ecossistema de Bibliotecas**: Python possui um vasto ecossistema de bibliotecas para manipula√ß√£o, processamento e ingest√£o de dados. Isso inclui bibliotecas como Pandas, NumPy, SQLAlchemy, e muitas outras, que facilitam a manipula√ß√£o de dados em diferentes formatos.

- **Integra√ß√£o com Servi√ßos de Nuvem**: Python oferece suporte a v√°rias bibliotecas e SDKs para integra√ß√£o com servi√ßos de nuvem, como AWS, Google Cloud e Azure. Isso permite que os desenvolvedores construam pipelines de ingest√£o de dados que se integram facilmente com servi√ßos de armazenamento em nuvem, como Amazon S3, Google Cloud Storage e Azure Data Lake Storage.

- **Facilidade de Desenvolvimento**: Python √© conhecido por sua sintaxe simples e leg√≠vel, o que facilita o desenvolvimento e a manuten√ß√£o de pipelines de ingest√£o de dados. Al√©m disso, existem frameworks como Apache Airflow e Prefect que podem ser usados para orquestrar e agendar pipelines de ingest√£o de dados de forma eficiente.

- **Comunidade Ativa**: Python possui uma comunidade de desenvolvedores muito ativa, o que significa que h√° uma grande quantidade de recursos dispon√≠veis online, incluindo tutoriais, documenta√ß√£o e f√≥runs de discuss√£o, que podem ajudar os desenvolvedores a resolver problemas e aprender novas t√©cnicas de ingest√£o de dados.

- **Portabilidade**: Python √© uma linguagem de programa√ß√£o port√°til, o que significa que os pipelines de ingest√£o de dados desenvolvidos em Python podem ser executados em uma variedade de ambientes, incluindo localmente em m√°quinas de desenvolvimento, em servidores de produ√ß√£o e em ambientes de nuvem.


## Como Executar
1. Clone o reposit√≥rio:
    ```sh
    git clone https://github.com/levyvix/big_data_ada_databricks.git
    ```
2. Navegue at√© o diret√≥rio do projeto:
    ```sh
    cd big_data_ada_databricks
    ```
3. Instale as depend√™ncias usando `poetry`:
    ```sh
    poetry install
    ```
4. Renomeie o arquivo `example.env` para `.env` e Preencha o arquivo `.env` com suas informa√ß√µes.
5. Execute o c√≥digo principal para carregar o arquivo `.csv` na sua pasta S3:
    ```sh
    python src/main.py
    ```
6. Execute os notebooks na sua inst√¢ncia do Databricks.

## Contribui√ß√µes
Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues ou enviar pull requests.

## Licen√ßa
Este projeto est√° licenciado sob a Licen√ßa Apache 2.0. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## Contato
Para d√∫vidas ou sugest√µes, abra uma issue no reposit√≥rio ou entre em contato diretamente.

---

Explore o c√≥digo, fa√ßa experimentos e aprenda mais sobre Big Data com AWS e Databricks. Boa sorte!
